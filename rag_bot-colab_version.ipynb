{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WPs5IfxCImG",
        "outputId": "17665a27-c236-44d2-ba69-41b765ec6da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install ollama chromadb --quiet\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bBaKyuiSCImI"
      },
      "outputs": [],
      "source": [
        "import ollama as olm\n",
        "import chromadb as cdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vWUJkZo9CImI"
      },
      "outputs": [],
      "source": [
        "# Import the Narnia books as documents\n",
        "\n",
        "import os\n",
        "\n",
        "def read_all_lines(dir_path:str, files:list) :\n",
        "  books = [[] for x in files]\n",
        "  for f_name, b in zip(tqdm(files), books) :\n",
        "    full_path = os.path.join(dir_path, f_name)\n",
        "    with open(full_path, 'r', encoding='unicode_escape') as f :\n",
        "      i = 0\n",
        "      while f.readable() and i < 10000:\n",
        "        i += 1\n",
        "        read_text = f.readline()\n",
        "        if len(read_text) == 0 :\n",
        "          break\n",
        "        else :\n",
        "          b.append({\"text\" : read_text, 'file' : f_name})\n",
        "  return books\n",
        "\n",
        "# print(*books[6][0:40])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hfMUwNNuCImJ"
      },
      "outputs": [],
      "source": [
        "# Optional addition of chapter and para metadata\n",
        "import re\n",
        "\n",
        "def add_chap_para_data(book_lines:list) :\n",
        "  chapter_start_matcher = re.compile(\"[^a-zA-Z]*chapter\", re.IGNORECASE)\n",
        "\n",
        "  for b in tqdm(book_lines) :\n",
        "    chapter_num = 0\n",
        "    para_num = 0\n",
        "    last_line_was_empty = True\n",
        "    for line in b :\n",
        "      if chapter_start_matcher.match(line['text']) is not None and last_line_was_empty :\n",
        "        chapter_num += 1\n",
        "        para_num = -1\n",
        "      else :\n",
        "        if last_line_was_empty :\n",
        "          para_num += 1\n",
        "      line['chapter'] = chapter_num\n",
        "      line['paragraph'] = para_num\n",
        "      last_line_was_empty = ( len(line['text'].lstrip()) == 0 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DbDtV1WQCImK"
      },
      "outputs": [],
      "source": [
        "def compile_into_snippets(book_lines:list, line_stride:int = 10, line_overlap:int = 2) :\n",
        "  line_jump = line_stride - line_overlap\n",
        "  snippets_per_book = [\n",
        "    [\n",
        "      {\n",
        "        'text' : ' '.join( [\n",
        "                  x['text'] for x in b[i_start:i_end]\n",
        "                ] ),\n",
        "        'file' : b[i_start]['file'],\n",
        "        'chap-para' : [\n",
        "          (b[i_start]['chapter'], b[i_start]['paragraph']),\n",
        "          (b[i_end]['chapter'], b[i_end]['paragraph'])\n",
        "        ],\n",
        "      }\n",
        "      for i_start in range(0,len(b),line_jump)\n",
        "      for i_end in [min(i_start + line_stride, len(b)-1)]\n",
        "    ]\n",
        "    for b in book_lines\n",
        "  ]\n",
        "  documents = []\n",
        "  for snips in snippets_per_book :\n",
        "    documents.extend(snips)\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "St2GnVNCCImK"
      },
      "outputs": [],
      "source": [
        "def create_embeddings_and_store(documents:list, collection, embed_model:str) :\n",
        "  olm.pull(model=embed_model)\n",
        "\n",
        "  print(\"pulled embedding model\")\n",
        "\n",
        "  olm_embeddings = [None] * len(documents)\n",
        "  tagged_text = [None] * len(documents)\n",
        "  for i, doc in enumerate(tqdm(documents)) :\n",
        "    # Add tags of filename and chapter/para\n",
        "    prefix = doc['file'] + ' '\n",
        "    chap_para = doc['chap-para']\n",
        "    # Same Chapter and Para\n",
        "    if len(set(chap_para)) == 1 :\n",
        "      prefix += 'Chapter %d, Para %d' % chap_para[0]\n",
        "    # Same Chapter, diff para\n",
        "    elif len({x[0] for x in chap_para}) == 1 :\n",
        "      chap = chap_para[0][0]\n",
        "      para1,para2 = chap_para[0][1], chap_para[1][1]\n",
        "      prefix += 'Chapter %d, Para %d to Para %d' % (chap, para1, para2)\n",
        "    # Diff Chapter, diff para\n",
        "    else :\n",
        "      (chap1, para1), (chap2, para2) = chap_para\n",
        "      prefix += 'Chapter %d, Para %d to Chapter %d, Para %d' % (chap1, para1, chap2, para2)\n",
        "    prefix += ' : '\n",
        "    tagged_text[i] = prefix + doc['text']\n",
        "\n",
        "    res = olm.embeddings(\n",
        "      model=embed_model,\n",
        "      prompt=tagged_text[i],\n",
        "    )\n",
        "    olm_embeddings[i] = res['embedding']\n",
        "\n",
        "  print(\"created embeddings\")\n",
        "\n",
        "  prev_count = collection.count()\n",
        "\n",
        "  for i, (emb, doc, text) in enumerate(zip(tqdm(olm_embeddings), documents, tagged_text)) :\n",
        "    metadata = doc.copy()\n",
        "    del metadata['text']\n",
        "    metadata['chap-para'] = \"%d,%d to %d,%d\" % (metadata['chap-para'][0][0],\n",
        "                                                metadata['chap-para'][0][1],\n",
        "                                                metadata['chap-para'][1][0],\n",
        "                                                metadata['chap-para'][1][1])\n",
        "    collection.add(\n",
        "      embeddings=[emb],\n",
        "      documents=[text],\n",
        "      metadatas=[metadata],\n",
        "      ids= [str(prev_count + i)],\n",
        "    )\n",
        "\n",
        "  print(\"added to collection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gmtaObzVCImK"
      },
      "outputs": [],
      "source": [
        "def answer_with_context(query:str, collection, answering_model:str, embed_model:str, top_k:int=20) :\n",
        "  olm.pull(model=answering_model)\n",
        "  print(\"pulled answering model\")\n",
        "\n",
        "  q_embed = olm.embeddings(\n",
        "    model=embed_model,\n",
        "    prompt=query\n",
        "  )['embedding']\n",
        "\n",
        "  print(\"querying knowledge base... \", end='')\n",
        "\n",
        "  context = collection.query(\n",
        "    query_embeddings=[q_embed],\n",
        "    n_results = top_k\n",
        "  )\n",
        "  # return context\n",
        "  context_ids = context['ids'][0]\n",
        "  context_str = context['documents'][0]\n",
        "  context_dis = context['distances'][0]\n",
        "  # Using cosine sim\n",
        "  relevant_context = [x for x, dis in zip(context_str,context_dis) if dis < 0.5]\n",
        "  all_context = '\\n\\n'.join(relevant_context)\n",
        "\n",
        "  print(\"query complete\")\n",
        "  print(\"Generating response... \", end='')\n",
        "\n",
        "  answer = olm.generate(\n",
        "    model = answering_model,\n",
        "    prompt = f\"Using the given context : \\n\\n {all_context} \\n\\n Answer the following question. Question {query} Answer :\"\n",
        "  )\n",
        "\n",
        "  print(\"Answer generated\")\n",
        "\n",
        "  return query, answer['response'], relevant_context"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "gutenberg_links = [\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-magiciansnephew/lewiscs-magiciansnephew-00-t.txt\",\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-thelionthewitchandthewardrobe/lewiscs-thelionthewitchandthewardrobe-00-t.txt\",\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-thehorseandhisboy/lewiscs-thehorseandhisboy-00-t.txt\",\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-princecaspian/lewiscs-princecaspian-00-t.txt\",\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-voyageofthedawntreader/lewiscs-voyageofthedawntreader-00-t.txt\",\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-silverchair/lewiscs-silverchair-00-t.txt\",\n",
        "    \"https://gutenberg.ca/ebooks/lewiscs-lastbattle/lewiscs-lastbattle-00-t.txt\",\n",
        "]\n",
        "\n",
        "narnia_path = 'narnia_series'\n",
        "for f in tqdm(gutenberg_links) :\n",
        "  if not os.path.exists(narnia_path) :\n",
        "    os.mkdir(narnia_path)\n",
        "  urllib.request.urlretrieve(f, os.path.join(narnia_path,f.split('/')[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P-rwKnWCVXi",
        "outputId": "e193421b-b6c6-4bd7-cfaf-63a575799633"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgix_jhECImK",
        "outputId": "3a3ec0bb-533e-43f9-a393-34b392f4a446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 21.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " * A Project Gutenberg Canada Ebook *\n",
            " \n",
            " This ebook is made available at no cost and with very few\n",
            " restrictions. These restrictions apply only if (1) you make\n",
            " a change in the ebook (other than alteration for different\n",
            " display devices), or (2) you are making commercial use of\n",
            " the ebook. If either of these conditions applies, please\n",
            " check gutenberg.ca/links/licence.html before proceeding.\n",
            " \n",
            " This work is in the Canadian public domain, but may be\n",
            " under copyright in some countries. If you live outside Canada,\n",
            " check your country's copyright laws. IF THE BOOK IS UNDER\n",
            " COPYRIGHT IN YOUR COUNTRY, DO NOT DOWNLOAD\n",
            " OR REDISTRIBUTE THIS FILE.\n",
            " \n",
            " Title: The Lion, the Witch and the Wardrobe.\n",
            "   A Story for Children.\n",
            " Author: Lewis, C. S. [Clive Staples] (1898-1963)\n",
            " Date of first publication: 1950\n",
            " Edition used as base for this ebook:\n",
            "   New York: Macmillan, undated\n",
            "   [twenty-first printing]\n",
            " Date first posted: 26 January 2014\n",
            " Date last updated: 26 January 2014\n",
            " Project Gutenberg Canada ebook #1152\n",
            " \n",
            " This ebook was produced by Al Haines\n",
            " \n",
            " \n",
            " [Transcriber's note: Because of copyright considerations,\n",
            " the illustrations by Pauline Baynes (1922-2008) have been\n",
            " omitted from this etext.]\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   THE LION, THE WITCH\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "client = cdb.PersistentClient(path=\"narnia_db\")\n",
        "narnia_db = {\"name\" : \"narnia_knowledge_base\"}\n",
        "narnia_db[\"db\"] = client.get_or_create_collection(name=narnia_db['name'],metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "files = os.listdir('narnia_series')\n",
        "\n",
        "trimmed_files = [x for x in files if x.endswith(\".txt\")]\n",
        "\n",
        "book_lines = read_all_lines(narnia_path, trimmed_files)\n",
        "print(*[x['text'] for x in book_lines[6][0:40]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4fPeUYCImK",
        "outputId": "2efb3586-64a7-41c0-9a14-2f554402ba26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 371.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '\"But what have you done?\" asked Lucy.\\n',\n",
            " 'file': 'lewiscs-thelionthewitchandthewardrobe-00-t.txt',\n",
            " 'chapter': 2,\n",
            " 'paragraph': 34}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install pprintpp --quiet\n",
        "from pprint import pp\n",
        "\n",
        "print()\n",
        "\n",
        "add_chap_para_data(book_lines)\n",
        "pp(book_lines[6][420])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHAmX47OCImL",
        "outputId": "d2444115-a994-4ecf-d26a-a1c39b86a560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4478\n",
            "{'text': \"chosen councillors was quite close.  Digory knew that he couldn't\\n\"\n",
            "         ' possibly break in on so solemn a meeting, but there was no need to '\n",
            "         'do\\n'\n",
            "         ' so.  At a word from Aslan, the He-Elephant, the Ravens, and all '\n",
            "         'the\\n'\n",
            "         ' rest of them drew aside.  Digory slipped off the horse and found\\n'\n",
            "         ' himself face to face with Aslan.  And Aslan was bigger and more\\n'\n",
            "         ' beautiful and more brightly golden and more terrible than he had\\n'\n",
            "         ' thought.  He dared not look into the great eyes.\\n'\n",
            "         ' \\n'\n",
            "         ' \"Please--Mr. Lion--Aslan--Sir?\" said Digory, \"could you--may '\n",
            "         'I--please,\\n'\n",
            "         ' will you give me some magic fruit of this country to make Mother '\n",
            "         'well?\"\\n',\n",
            " 'file': 'lewiscs-magiciansnephew-00-t.txt',\n",
            " 'chap-para': [(11, 31), (11, 32)]}\n"
          ]
        }
      ],
      "source": [
        "snippets = compile_into_snippets(book_lines)\n",
        "print(len(snippets))\n",
        "pp(snippets[420])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZmDwK2VUjVe",
        "outputId": "7ed9c0a7-31d9-4caf-82ba-f8cb249bddbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvUlhQ8jiNNa",
        "outputId": "aa00d414-b672-4a41-bd51-116c14edb7e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-xterm\n",
            "  Downloading colab_xterm-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (6.3.3)\n",
            "Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: colab-xterm\n",
            "Successfully installed colab-xterm-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve & disown\n",
        "# !ollama serve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lggkBSRwiTg8",
        "outputId": "36b1f387-3bab-4ea0-f32f-7677a545f6c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USo_gl_kCImL",
        "outputId": "e91d48a7-f593-4950-a41f-eb4d73ea3c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pulled embedding model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4478/4478 [06:30<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4478/4478 [00:51<00:00, 87.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added to collection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "answering_model = \"gemma2\"\n",
        "embed_model = \"mxbai-embed-large\"\n",
        "\n",
        "# We don't want to add again and again\n",
        "if narnia_db['db'].count() == 0 :\n",
        "  create_embeddings_and_store(snippets, narnia_db['db'], embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ESIFnCTyCImL"
      },
      "outputs": [],
      "source": [
        "responses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DhGVXmdvCImL",
        "outputId": "31a9ffa5-32ea-4937-88ac-bbb87e9503e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pulled answering model\n",
            "querying knowledge base... query complete\n",
            "Generating response... Answer generated\n",
            "\n",
            "Question : Who is Eustace? \n",
            " Answer : Eustace is a character in C.S. Lewis's fantasy series *The Chronicles of Narnia*. He is a  young boy who starts off as grumpy and unpleasant but undergoes a transformation during his adventures in Narnia. \n",
            "\n",
            "\n",
            "Let me know if you have any other questions about Eustace or The Chronicles of Narnia!\n"
          ]
        }
      ],
      "source": [
        "responses.append(answer_with_context(\"Who is Eustace?\", narnia_db['db'], answering_model, embed_model))\n",
        "print()\n",
        "print(\"Question : %s \\n Answer : %s\" % (responses[-1][0], responses[-1][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MHZZ3DbqCImL",
        "outputId": "f0b8848d-35a8-4f96-8461-62ed68534997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pulled answering model\n",
            "querying knowledge base... query complete\n",
            "Generating response... Answer generated\n",
            "\n",
            "Question : Who is Jadis? \n",
            " Answer : Based on the provided text, Jadis is a powerful and fearsome witch.  \n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **She is referred to as \"the Queen of Queens and the Terror of Charn.\"** This suggests she holds a high position and is known for her ruthlessness.\n",
            "* **Her appearance is described as menacing:** She bares her teeth, her eyes shine like fire, and her hair streams out behind her like a comet's tail.\n",
            "* **She treats her horse cruelly,** flogging it mercilessly. \n",
            "* **She possesses magical abilities** allowing her to jump clear of a crashing hansom cab and seemingly communicate with the horse telepathically.\n",
            "* **Her voice is powerful enough to make a room quiver.**\n",
            "\n",
            "\n",
            "All of these details paint a picture of Jadis as a formidable and potentially dangerous figure.  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "responses.append(answer_with_context(\"Who is Jadis?\", narnia_db['db'], answering_model, embed_model))\n",
        "print()\n",
        "print(\"Question : %s \\n Answer : %s\" % (responses[-1][0], responses[-1][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b83T2cqaCImL",
        "outputId": "9dce7c6f-c742-4507-9739-2dfbc733830f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pulled answering model\n",
            "querying knowledge base... query complete\n",
            "Generating response... Answer generated\n",
            "\n",
            "Question : In what chapter, para and book was Susan called 'not a friend of Narnia'? \n",
            " Answer : Susan was called \"not a friend of Narnia\" in **Chapter 12, Paragraph 51** of  **The Last Battle**. \n",
            "\n",
            "\n",
            "Let me know if you have any other questions about these C.S. Lewis excerpts! 😊 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Direct questions require fewer RAG results\n",
        "responses.append(answer_with_context(\"In what chapter, para and book was Susan called 'not a friend of Narnia'?\", narnia_db['db'], answering_model, embed_model,top_k=10))\n",
        "print()\n",
        "print(\"Question : %s \\n Answer : %s\" % (responses[-1][0], responses[-1][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sGw6F4kJCImL",
        "outputId": "c9c6dea8-e48d-42d3-9f7b-8e018e10509b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pulled answering model\n",
            "querying knowledge base... query complete\n",
            "Generating response... Answer generated\n",
            "\n",
            "Question : Who is Ramandu's daughter? \n",
            " Answer : The provided text states that Caspian married Ramandu's daughter and she became a great queen in Narnia.  \n",
            "\n",
            "\n",
            "Let me know if you have any other questions from this excerpt! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "responses.append(answer_with_context(\"Who is Ramandu's daughter?\", narnia_db['db'], answering_model, embed_model))\n",
        "print()\n",
        "print(\"Question : %s \\n Answer : %s\" % (responses[-1][0], responses[-1][1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eHOKLF1vypg-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}